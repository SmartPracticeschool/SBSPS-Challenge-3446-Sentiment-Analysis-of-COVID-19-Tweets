# -*- coding: utf-8 -*-
"""Simple_CNN_for_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WkhYUC93_IZO-g74sFu0Pv-ynBd7Qo_4

# Stage 1:IMPORTING DEPENDENCIES
"""

import numpy as np
import math
import re
import time
import pandas as pd
from bs4 import BeautifulSoup
from google.colab import drive

# Commented out IPython magic to ensure Python compatibility.
try:
#     %tensorflow_version 2.x
except:
    pass
import tensorflow as tf

from tensorflow.keras import layers
import tensorflow_datasets as tfds

"""# Stage 2:Data Preprocessing

### Loading Files
"""

drive.mount("/content/drive")

cols = ["sentiment", "id","date","query","user","text"]
train_data=pd.read_csv(
    "/content/drive/My Drive/projects/CNN_for_NLP/data/train.csv",
    header=None,
    names=cols,
    engine="python",
    encoding="latin1"
    )
test_data=pd.read_csv(
    "/content/drive/My Drive/projects/CNN_for_NLP/data/test.csv",
    header=None,
    names=cols,
    engine="python",
    encoding="latin1"
    )
data=train_data

print(train_data)

"""### Preprocessing

### Cleaning
"""

data.drop(["id","date","query","user"],
          axis=1,
          inplace=True)

def clean_tweet(tweet):
    tweet = BeautifulSoup(tweet, "lxml").get_text()
    tweet = re.sub(r"@[A-Za-z0-9]+",' ', tweet)
    tweet = re.sub(r"https?://[A-Za-z0-9./]+",' ', tweet)
    tweet = re.sub(r"[^a-zA-Z.!?']",' ', tweet)
    tweet = re.sub(r" +"," ", tweet)
    return tweet

data_clean=[clean_tweet(tweet) for tweet in data.text]

data_labels=data.sentiment.values
data_labels[data_labels==4]=1

set(data_labels)

"""### Tokenization"""

tokenizer= tfds.features.text.SubwordTextEncoder.build_from_corpus(
    data_clean,target_vocab_size=2**16
) 

data_inputs=[tokenizer.encode(sentence) for sentence in data_clean]

print(tokenizer)
type(tokenizer)

"""### Padding"""

MAX_LEN=max([len(sentence) for sentence in data_inputs])
data_inputs= tf.keras.preprocessing.sequence.pad_sequences(data_inputs,
                                                           value=0,
                                                           padding="post",
                                                           maxlen=MAX_LEN)

"""### Splitting into Training/Testing Sets"""

test_idx = np.random.randint(0, 800000, 8000)
text_idx = np.concatenate((test_idx,test_idx+800000))

test_inputs = data_inputs[test_idx]
test_labels = data_labels[test_idx]
train_inputs = np.delete(data_inputs,test_idx,axis=0)
train_labels = np.delete(data_labels,test_idx)

"""# Stage 3 : Model Building"""

class DCNN(tf.keras.Model):

    def __init__(self,
                 vocab_size,
                 emb_dim=128,
                 nb_filters=50,
                 FFN_units=512,
                 nb_classes=2,
                 dropout_rate=0.1,
                 training=False,
                 name="dcnn"):
        super(DCNN, self).__init__(name=name)

        self.embedding=layers.Embedding(vocab_size,emb_dim)

        self.bigram= layers.Conv1D(filters=nb_filters,
                                   kernel_size=2,
                                   padding="valid",
                                   activation="relu")         
        self.pool_1 = layers.GlobalMaxPool1D() 

        self.trigram= layers.Conv1D(filters=nb_filters,
                                    kernel_size=3,
                                    padding="valid",
                                    activation="relu")
        self.pool_2 = layers.GlobalMaxPool1D() 

        self.fourgram= layers.Conv1D(filters=nb_filters,
                                     kernel_size=4,
                                     padding="valid",
                                     activation="relu")  
        self.pool_3 = layers.GlobalMaxPool1D()
         
        self.dense_1= layers.Dense(units=FFN_units,activation="relu")
        self.dropout= layers.Dropout(rate=dropout_rate)
        if nb_classes == 2:
            self.last_dense= layers.Dense(units=1,
                                          activation="sigmoid")
        else:
            self.last_dense= layers.Dense(units=nb_classes,
                                          activation="softmax")
    def call(self, inputs,training):
        x= self.embedding(inputs)
        x_1=self.bigram(x)
        x_1=self.pool_1(x_1)
        x_2=self.trigram(x)
        x_2=self.pool_1(x_2)
        x_3=self.fourgram(x)
        x_3=self.pool_1(x_3)    

        merged = tf.concat([x_1,x_2,x_3], axis=-1)  # (batch_size, 3 * nb_filters)
        merged = self.dense_1(merged)
        merged = self.dropout(merged, training)
        output = self.last_dense(merged)
        return output

"""# Stage 4: Application

### Config
"""

VOCAB_SIZE = tokenizer.vocab_size

EMB_DIM = 200
NB_FILTERS = 100
FFN_UNITS = 256
NB_CLASSES = len(set(train_labels))

DROPOUT_RATE = 0.1

BATCH_SIZE = 32
NB_EPOCHS = 5

"""### Training"""

Dcnn = DCNN(vocab_size=VOCAB_SIZE,
            emb_dim=EMB_DIM,
            nb_filters=NB_FILTERS,
            FFN_units=FFN_UNITS,
            nb_classes=NB_CLASSES,
            dropout_rate=DROPOUT_RATE 
            )

if NB_CLASSES==2:
    Dcnn.compile(loss="binary_crossentropy",
                 optimizer="adam",
                 metrics=["accuracy"])
else:
    Dcnn.compile(loss="sparse_categorical_crossentropy",
                 optimizer="adam",
                 metrics=["sparse_categorical_accuracy"])

checkpoint_path="./drive/My Drive/projects/CNN_for_NLP/ckpt/"

ckpt= tf.train.Checkpoint(Dcnn=Dcnn)

ckpt_manager = tf.train.CheckpointManager(ckpt,checkpoint_path, max_to_keep=1)

if ckpt_manager.latest_checkpoint:
    ckpt.restore(ckpt_manager.latest_checkpoint)
    print("Latest Checkpoint Restored")

Dcnn.fit(train_inputs,
         train_labels,
         batch_size=BATCH_SIZE,
         epochs=NB_EPOCHS
         )
ckpt_manager.save()

"""### Evaluation"""

results = Dcnn.evaluate(test_inputs,test_labels,batch_size=BATCH_SIZE)
print(results)

s=input()
print(Dcnn(np.array([tokenizer.encode(s)]),training=False).numpy())
if(Dcnn(np.array([tokenizer.encode(s)]),training=False).numpy()> 0.500000):
    print("positive")
else:
    print("negative")

"""# User Interface

### Graph
"""

import re
import io
import csv
import tweepy
from tweepy import OAuthHandler
#TextBlob perform simple natural language processing tasks.
from textblob import TextBlob
import pandas as pd
import numpy as np
from pandas import DataFrame
import matplotlib.pyplot as plt

def graphcodes(): 

    consumer_key = "6SdDJk667wCwH60Efafw0ho59" 
    consumer_secret = "M2XGvqZ3W1SYNvtJaYSjf9vXqO9X4c1ulePOKFUxUSohGK2QMC"
    access_key = "1280716155225731073-lwUmSwyRrQGFMZAcrFaOf7LgzScUYm"
    access_secret = "NB2Saytc2Jlqk7Z9ZgSA5CBqM7xeWURbP4UfdNGEMsJKc"

    # create OAuthHandler object
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    # set access token and secret
    auth.set_access_token(access_key, access_secret)
    # create tweepy API object to fetch tweets
    api = tweepy.API(auth)

    class TweetAnalyzer():
        """
        Functionality for analyzing and categorizing content from tweets.
        """

        def clean_tweet(self, tweet):
            return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())

        def analyze_sentiment(self, tweet):
            analysis = TextBlob(self.clean_tweet(tweet))
            
            if analysis.sentiment.polarity > 0:
                return 1
            elif analysis.sentiment.polarity == 0:
                return 0
            else:
                return -1

        def tweets_to_data_frame(self, tweets):
            df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])

            df['id'] = np.array([tweet.id for tweet in tweets])
            df['len'] = np.array([len(tweet.text) for tweet in tweets])
            df['date'] = np.array([tweet.created_at for tweet in tweets])
            df['source'] = np.array([tweet.source for tweet in tweets])
            df['likes'] = np.array([tweet.favorite_count for tweet in tweets])
            df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])

            return df


    def get_tweets(query, count=10000):

        # empty list to store parsed tweets
        tweets = []
        target = io.open("mytweets.txt", 'w', encoding='utf-8')
        # call twitter api to fetch tweets
        q=str(query)
        a=str(q+" corona virus")
        b=str(q+" Covid-19")
        c=str(q+" Corona")
        fetched_tweets = api.search(a, count = count)+ api.search(b, count = count)+ api.search(c, count = count)
        # parsing tweets one by one
        print()
        print()
        print()
        #for i in fetched_tweets:
        #    print(i.text)

        for tweet in fetched_tweets:

            # empty dictionary to store required params of a tweet
            parsed_tweet = {}
            # saving text of tweet
            parsed_tweet['text'] = tweet.text
            if "http" not in tweet.text:
                line = re.sub("[^A-Za-z]", " ", tweet.text)
                target.write(line+"\n")
        return fetched_tweets

        # creating object of TwitterClient Class
        # calling function to get tweets
        
    tweets = get_tweets(query ="", count = 10000)
    tweet_analyzer = TweetAnalyzer()
    df = tweet_analyzer.tweets_to_data_frame(tweets)
    df['sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])
    print(df.head(10000))
    one,zero,minus=0,0,0
    for i in df.sentiment:
        if(i==0):
            zero+=1
        elif(i==1):
            one+=1
        else:
            minus+=1
    print(zero,one,minus)
    return(zero,one,minus)

x,y,z= graphcodes()
print(x,y,z)

def piechart(x,y,z):
      data_pie=[]
      data_pie.append(x)
      data_pie.append(y)
      data_pie.append(z)

      colors = ( "Blue", "Red", "Green") 
      explode = (0.1,0.1,0.0)
      data_set=["Positive","Negative","Neutral"]
      wp = { 'linewidth' : 1, 'edgecolor' : "black" } 

      def func(pct, allvalues): 
          absolute = int(pct / 100.*np.sum(allvalues)) 
          return "{:.1f}%\n({:d} K)".format(pct, absolute) 
        
      # Creating plot 
      fig, ax = plt.subplots(figsize =(10, 5)) 
      wedges, texts, autotexts = ax.pie(data_pie,  
                                        autopct = lambda pct: func(pct, data_pie), 
                                        explode = explode,  
                                        labels = data_set, 
                                        shadow = True, 
                                        colors = colors, 
                                        startangle = 90, 
                                        wedgeprops = wp, 
                                        textprops = dict(color ="black")) 
        
      # Adding legend 
      ax.legend(wedges, data_set, 
                title ="Sentiment", 
                loc ="center left", 
                bbox_to_anchor =(1, 0, 0.5, 1)) 
        
      plt.setp(autotexts, size = 8, weight ="bold") 
      ax.set_title("Customizing pie chart") 
      return plt.show()

print(piechart(zero,one,minus))

def livetweet(nums):
    consumer_key = "6SdDJk667wCwH60Efafw0ho59" 
    consumer_secret = "M2XGvqZ3W1SYNvtJaYSjf9vXqO9X4c1ulePOKFUxUSohGK2QMC"
    access_key = "1280716155225731073-lwUmSwyRrQGFMZAcrFaOf7LgzScUYm"
    access_secret = "NB2Saytc2Jlqk7Z9ZgSA5CBqM7xeWURbP4UfdNGEMsJKc"

    # create OAuthHandler object
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    # set access token and secret
    auth.set_access_token(access_key, access_secret)
    # create tweepy API object to fetch tweets
    api = tweepy.API(auth)

    class TweetAnalyzer():
        """
        Functionality for analyzing and categorizing content from tweets.
        """

        def clean_tweet(self, tweet):
            return ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)", " ", tweet).split())

        def analyze_sentiment(self, tweet):
            analysis = TextBlob(self.clean_tweet(tweet))
            
            if analysis.sentiment.polarity > 0:
                return 1
            elif analysis.sentiment.polarity == 0:
                return 0
            else:
                return -1

        def tweets_to_data_frame(self, tweets):
            df = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['tweets'])

            df['id'] = np.array([tweet.id for tweet in tweets])
            df['len'] = np.array([len(tweet.text) for tweet in tweets])
            df['date'] = np.array([tweet.created_at for tweet in tweets])
            df['source'] = np.array([tweet.source for tweet in tweets])
            df['likes'] = np.array([tweet.favorite_count for tweet in tweets])
            df['retweets'] = np.array([tweet.retweet_count for tweet in tweets])

            return df


    def get_tweets(query, count=nums):

        # empty list to store parsed tweets
        tweets = []
        target = io.open("mytweets.txt", 'w', encoding='utf-8')
        # call twitter api to fetch tweets
        q=str(query)
        a=str(q+" corona virus")
        b=str(q+" Covid-19")
        c=str(q+" Corona")
        fetched_tweets = api.search(a, count = count)+ api.search(b, count = count)+ api.search(c, count = count)
        # parsing tweets one by one
        print()
        print()
        print()
        #for i in fetched_tweets:
        #    print(i.text)

        for tweet in fetched_tweets:

            # empty dictionary to store required params of a tweet
            parsed_tweet = {}
            # saving text of tweet
            parsed_tweet['text'] = tweet.text
            if "http" not in tweet.text:
                line = re.sub("[^A-Za-z]", " ", tweet.text)
                target.write(line+"\n")
        return fetched_tweets

        # creating object of TwitterClient Class
        # calling function to get tweets
        
    tweets = get_tweets(query ="", count = nums)
    tweet_analyzer = TweetAnalyzer()
    df = tweet_analyzer.tweets_to_data_frame(tweets)
    df['sentiment'] = np.array([tweet_analyzer.analyze_sentiment(tweet) for tweet in df['tweets']])
    s=""
    mnt=1
    for i in range(nums):
      s+=str(mnt)
      s+=". "
      s+=df.tweets[i]
      s+="\n"
      mnt+=1
    return(s)

print(livetweet(2))

"""### Anvil Server"""

pip install anvil-uplink

import anvil.server
anvil.server.connect("JJ7SNEGAU6X3HENXMVRHJFJE-VQZO4PGJSNCXQYTU")

@anvil.server.callable
def sentiment(sacc):
    limes=Dcnn(np.array([tokenizer.encode(sacc)]),training=False).numpy()
    if limes>= 0.700000:
      return ("POSITIVE :)" )
    elif (limes>0.300000 and limes<0.700000):
       return("NEUTRAL :|")
    else:
      return("NEGATIVE :(")

@anvil.server.callable
def graph(tacc):
    x,y,z= graphcodes()
    return(x,y,z)

@anvil.server.callable
def pie(x,y,z):
  r = piechart(x,y,z)
  return (r)

@anvil.server.callable
def display(nums):
  kill= livetweet(nums)
  return kill